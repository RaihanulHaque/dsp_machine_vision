{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63bf1370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with batch normalization\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ImprovedCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
    "        \n",
    "        # Global average pooling and classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63fa522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DETAILED MODEL SUMMARY ###\n",
      "====================================================================================================\n",
      "Layer (type)                             Output Shape              Param #        \n",
      "====================================================================================================\n",
      "Conv2d-1                                 [1, 64, 32, 32]                     1,728\n",
      "BatchNorm2d-2                            [1, 64, 32, 32]                       128\n",
      "Conv2d-3                                 [1, 64, 32, 32]                    36,864\n",
      "BatchNorm2d-4                            [1, 64, 32, 32]                       128\n",
      "Conv2d-5                                 [1, 64, 32, 32]                    36,864\n",
      "BatchNorm2d-6                            [1, 64, 32, 32]                       128\n",
      "ResidualBlock-7                          [1, 64, 32, 32]                         0\n",
      "Conv2d-8                                 [1, 64, 32, 32]                    36,864\n",
      "BatchNorm2d-9                            [1, 64, 32, 32]                       128\n",
      "Conv2d-10                                [1, 64, 32, 32]                    36,864\n",
      "BatchNorm2d-11                           [1, 64, 32, 32]                       128\n",
      "ResidualBlock-12                         [1, 64, 32, 32]                         0\n",
      "Conv2d-13                                [1, 128, 16, 16]                   73,728\n",
      "BatchNorm2d-14                           [1, 128, 16, 16]                      256\n",
      "Conv2d-15                                [1, 128, 16, 16]                  147,456\n",
      "BatchNorm2d-16                           [1, 128, 16, 16]                      256\n",
      "Conv2d-17                                [1, 128, 16, 16]                    8,192\n",
      "BatchNorm2d-18                           [1, 128, 16, 16]                      256\n",
      "ResidualBlock-19                         [1, 128, 16, 16]                        0\n",
      "Conv2d-20                                [1, 128, 16, 16]                  147,456\n",
      "BatchNorm2d-21                           [1, 128, 16, 16]                      256\n",
      "Conv2d-22                                [1, 128, 16, 16]                  147,456\n",
      "BatchNorm2d-23                           [1, 128, 16, 16]                      256\n",
      "ResidualBlock-24                         [1, 128, 16, 16]                        0\n",
      "Conv2d-25                                [1, 256, 8, 8]                    294,912\n",
      "BatchNorm2d-26                           [1, 256, 8, 8]                        512\n",
      "Conv2d-27                                [1, 256, 8, 8]                    589,824\n",
      "BatchNorm2d-28                           [1, 256, 8, 8]                        512\n",
      "Conv2d-29                                [1, 256, 8, 8]                     32,768\n",
      "BatchNorm2d-30                           [1, 256, 8, 8]                        512\n",
      "ResidualBlock-31                         [1, 256, 8, 8]                          0\n",
      "Conv2d-32                                [1, 256, 8, 8]                    589,824\n",
      "BatchNorm2d-33                           [1, 256, 8, 8]                        512\n",
      "Conv2d-34                                [1, 256, 8, 8]                    589,824\n",
      "BatchNorm2d-35                           [1, 256, 8, 8]                        512\n",
      "ResidualBlock-36                         [1, 256, 8, 8]                          0\n",
      "Conv2d-37                                [1, 512, 4, 4]                  1,179,648\n",
      "BatchNorm2d-38                           [1, 512, 4, 4]                      1,024\n",
      "Conv2d-39                                [1, 512, 4, 4]                  2,359,296\n",
      "BatchNorm2d-40                           [1, 512, 4, 4]                      1,024\n",
      "Conv2d-41                                [1, 512, 4, 4]                    131,072\n",
      "BatchNorm2d-42                           [1, 512, 4, 4]                      1,024\n",
      "ResidualBlock-43                         [1, 512, 4, 4]                          0\n",
      "Conv2d-44                                [1, 512, 4, 4]                  2,359,296\n",
      "BatchNorm2d-45                           [1, 512, 4, 4]                      1,024\n",
      "Conv2d-46                                [1, 512, 4, 4]                  2,359,296\n",
      "BatchNorm2d-47                           [1, 512, 4, 4]                      1,024\n",
      "ResidualBlock-48                         [1, 512, 4, 4]                          0\n",
      "AdaptiveAvgPool2d-49                     [1, 512, 1, 1]                          0\n",
      "Linear-50                                [1, 10]                             5,130\n",
      "====================================================================================================\n",
      "Total params: 11,173,962\n",
      "Trainable params: 11,173,962\n",
      "Non-trainable params: 0\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "### PARAMETER SUMMARY BY LAYER ###\n",
      "\n",
      "============================================================\n",
      "                 Model Architecture Summary                 \n",
      "============================================================\n",
      "\n",
      "Layer                               Parameters    Trainable\n",
      "------------------------------------------------------------\n",
      "conv1.weight                             1,728          Yes\n",
      "bn1.weight                                  64          Yes\n",
      "bn1.bias                                    64          Yes\n",
      "layer1.0.conv1.weight                   36,864          Yes\n",
      "layer1.0.bn1.weight                         64          Yes\n",
      "layer1.0.bn1.bias                           64          Yes\n",
      "layer1.0.conv2.weight                   36,864          Yes\n",
      "layer1.0.bn2.weight                         64          Yes\n",
      "layer1.0.bn2.bias                           64          Yes\n",
      "layer1.1.conv1.weight                   36,864          Yes\n",
      "layer1.1.bn1.weight                         64          Yes\n",
      "layer1.1.bn1.bias                           64          Yes\n",
      "layer1.1.conv2.weight                   36,864          Yes\n",
      "layer1.1.bn2.weight                         64          Yes\n",
      "layer1.1.bn2.bias                           64          Yes\n",
      "layer2.0.conv1.weight                   73,728          Yes\n",
      "layer2.0.bn1.weight                        128          Yes\n",
      "layer2.0.bn1.bias                          128          Yes\n",
      "layer2.0.conv2.weight                  147,456          Yes\n",
      "layer2.0.bn2.weight                        128          Yes\n",
      "layer2.0.bn2.bias                          128          Yes\n",
      "layer2.0.shortcut.0.weight               8,192          Yes\n",
      "layer2.0.shortcut.1.weight                 128          Yes\n",
      "layer2.0.shortcut.1.bias                   128          Yes\n",
      "layer2.1.conv1.weight                  147,456          Yes\n",
      "layer2.1.bn1.weight                        128          Yes\n",
      "layer2.1.bn1.bias                          128          Yes\n",
      "layer2.1.conv2.weight                  147,456          Yes\n",
      "layer2.1.bn2.weight                        128          Yes\n",
      "layer2.1.bn2.bias                          128          Yes\n",
      "layer3.0.conv1.weight                  294,912          Yes\n",
      "layer3.0.bn1.weight                        256          Yes\n",
      "layer3.0.bn1.bias                          256          Yes\n",
      "layer3.0.conv2.weight                  589,824          Yes\n",
      "layer3.0.bn2.weight                        256          Yes\n",
      "layer3.0.bn2.bias                          256          Yes\n",
      "layer3.0.shortcut.0.weight              32,768          Yes\n",
      "layer3.0.shortcut.1.weight                 256          Yes\n",
      "layer3.0.shortcut.1.bias                   256          Yes\n",
      "layer3.1.conv1.weight                  589,824          Yes\n",
      "layer3.1.bn1.weight                        256          Yes\n",
      "layer3.1.bn1.bias                          256          Yes\n",
      "layer3.1.conv2.weight                  589,824          Yes\n",
      "layer3.1.bn2.weight                        256          Yes\n",
      "layer3.1.bn2.bias                          256          Yes\n",
      "layer4.0.conv1.weight                1,179,648          Yes\n",
      "layer4.0.bn1.weight                        512          Yes\n",
      "layer4.0.bn1.bias                          512          Yes\n",
      "layer4.0.conv2.weight                2,359,296          Yes\n",
      "layer4.0.bn2.weight                        512          Yes\n",
      "layer4.0.bn2.bias                          512          Yes\n",
      "layer4.0.shortcut.0.weight             131,072          Yes\n",
      "layer4.0.shortcut.1.weight                 512          Yes\n",
      "layer4.0.shortcut.1.bias                   512          Yes\n",
      "layer4.1.conv1.weight                2,359,296          Yes\n",
      "layer4.1.bn1.weight                        512          Yes\n",
      "layer4.1.bn1.bias                          512          Yes\n",
      "layer4.1.conv2.weight                2,359,296          Yes\n",
      "layer4.1.bn2.weight                        512          Yes\n",
      "layer4.1.bn2.bias                          512          Yes\n",
      "fc.weight                                5,120          Yes\n",
      "fc.bias                                     10          Yes\n",
      "------------------------------------------------------------\n",
      "Total Parameters                    11,173,962\n",
      "Trainable Parameters                11,173,962\n",
      "Non-trainable Parameters                     0\n",
      "============================================================\n",
      "\n",
      "Estimated Model Size: 42.63 MB (float32)\n",
      "============================================================\n",
      "\n",
      "\n",
      "Quick Stats:\n",
      "  Total parameters: 11,173,962\n",
      "  Trainable parameters: 11,173,962\n"
     ]
    }
   ],
   "source": [
    "# Training configuration for better results\n",
    "def get_training_config():\n",
    "    \"\"\"Recommended training configuration\"\"\"\n",
    "    config = {\n",
    "        'learning_rate': 0.1,\n",
    "        'momentum': 0.9,\n",
    "        'weight_decay': 5e-4,\n",
    "        'batch_size': 128,\n",
    "        'epochs': 200,\n",
    "        'lr_scheduler': 'cosine',  # or 'multistep'\n",
    "        'warmup_epochs': 5,\n",
    "        'label_smoothing': 0.1,\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "# Data augmentation for training\n",
    "def get_transforms():\n",
    "    \"\"\"Recommended data augmentation\"\"\"\n",
    "    from torchvision import transforms\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        transforms.RandomErasing(p=0.5, scale=(0.02, 0.33)),\n",
    "    ])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    return train_transform, test_transform\n",
    "\n",
    "\n",
    "# Example training loop with improvements\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device, epoch, warmup_epochs=5):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Warmup learning rate\n",
    "        if epoch < warmup_epochs:\n",
    "            warmup_lr = 0.1 * (epoch * len(train_loader) + batch_idx) / (warmup_epochs * len(train_loader))\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = warmup_lr\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# Model summary functions\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count total and trainable parameters\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "\n",
    "def print_model_summary(model, input_size=(3, 32, 32), batch_size=1, device='cpu'):\n",
    "    \"\"\"\n",
    "    Print model summary similar to TensorFlow's model.summary()\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        input_size: Input tensor size (C, H, W)\n",
    "        batch_size: Batch size for forward pass\n",
    "        device: Device to run on\n",
    "    \"\"\"\n",
    "    def register_hook(module):\n",
    "        def hook(module, input, output):\n",
    "            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "            module_idx = len(summary)\n",
    "            \n",
    "            m_key = f\"{class_name}-{module_idx+1}\"\n",
    "            summary[m_key] = {}\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            summary[m_key][\"output_shape\"] = list(output.size()) if isinstance(output, torch.Tensor) else [list(o.size()) for o in output]\n",
    "            \n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size()))).item()\n",
    "                summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.bias.size()))).item()\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "        \n",
    "        if not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList) and not (module == model):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "    \n",
    "    # Create summary dict\n",
    "    summary = {}\n",
    "    hooks = []\n",
    "    \n",
    "    # Register hooks\n",
    "    model.apply(register_hook)\n",
    "    \n",
    "    # Make a forward pass\n",
    "    model.eval()\n",
    "    x = torch.randn(batch_size, *input_size).to(device)\n",
    "    model(x)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"{'Layer (type)':<40} {'Output Shape':<25} {'Param #':<15}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    \n",
    "    for layer in summary:\n",
    "        line = f\"{layer:<40} {str(summary[layer]['output_shape']):<25} {summary[layer]['nb_params']:>15,}\"\n",
    "        print(line)\n",
    "        total_params += summary[layer][\"nb_params\"]\n",
    "        if \"trainable\" in summary[layer]:\n",
    "            if summary[layer][\"trainable\"]:\n",
    "                trainable_params += summary[layer][\"nb_params\"]\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {trainable_params:,}\")\n",
    "    print(f\"Non-trainable params: {total_params - trainable_params:,}\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "\n",
    "def print_simple_summary(model):\n",
    "    \"\"\"Simple parameter count summary\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"{'Model Architecture Summary':^60}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    \n",
    "    print(f\"\\n{'Layer':<30} {'Parameters':>15} {'Trainable':>12}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        total_params += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "        \n",
    "        trainable_status = \"Yes\" if param.requires_grad else \"No\"\n",
    "        print(f\"{name:<30} {num_params:>15,} {trainable_status:>12}\")\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    print(f\"{'Total Parameters':<30} {total_params:>15,}\")\n",
    "    print(f\"{'Trainable Parameters':<30} {trainable_params:>15,}\")\n",
    "    print(f\"{'Non-trainable Parameters':<30} {(total_params - trainable_params):>15,}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Calculate model size\n",
    "    param_size = total_params * 4 / (1024**2)  # Assuming float32\n",
    "    print(f\"\\nEstimated Model Size: {param_size:.2f} MB (float32)\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create model\n",
    "    model = ImprovedCNN(num_classes=10)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(\"\\n### DETAILED MODEL SUMMARY ###\")\n",
    "    print_model_summary(model, input_size=(3, 32, 32), device=device)\n",
    "    \n",
    "    print(\"\\n\\n### PARAMETER SUMMARY BY LAYER ###\")\n",
    "    print_simple_summary(model)\n",
    "    \n",
    "    # Quick parameter count\n",
    "    total, trainable = count_parameters(model)\n",
    "    print(f\"\\nQuick Stats:\")\n",
    "    print(f\"  Total parameters: {total:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable:,}\")\n",
    "    \n",
    "    # # Alternative: Use torchsummary (if installed)\n",
    "    # try:\n",
    "    #     from torchsummary import summary\n",
    "    #     print(\"\\n\\n### TORCHSUMMARY OUTPUT ###\")\n",
    "    #     summary(model, (3, 32, 32))\n",
    "    # except ImportError:\n",
    "    #     print(\"\\n[Optional] Install torchsummary for more details: pip install torchsummary\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
